# pip install meteostat tqdm joblib
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from meteostat import Hourly, Point
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from tqdm import tqdm

tqdm.pandas()


# 1. Load crime dataset

crime_df = pd.read_csv("Crimes_-_2001_to_Present_20250410.csv", low_memory=False)
crime_df['Date'] = pd.to_datetime(crime_df['Date'])
crime_df = crime_df[['Date', 'Arrest', 'Latitude', 'Longitude', 'Community Area']]
crime_df.dropna(subset=['Date', 'Latitude', 'Longitude'], inplace=True)
crime_df['DateHour'] = crime_df['Date'].dt.floor('h')
crime_df['LatGrid'] = crime_df['Latitude'].round(2)
crime_df['LonGrid'] = crime_df['Longitude'].round(2)


# 2. Create location-hour grid
time_range = pd.date_range(start=crime_df['DateHour'].min(), end=crime_df['DateHour'].max(), freq='H')
df = crime_df[['DateHour', 'LatGrid', 'LonGrid']].drop_duplicates()

# 3. Pull weather data
station = Point(41.9742, -87.9073)
weather_df = Hourly(station, time_range.min(), time_range.max()).fetch().reset_index()
weather_df['DateHour'] = weather_df['time'].dt.floor('h')
df = df.merge(weather_df, on='DateHour', how='left')

# Fill missing weather data using forward-fill
df[['temp', 'rhum', 'prcp', 'snow', 'wspd']] = df[['temp', 'rhum', 'prcp', 'snow', 'wspd']].fillna(method='ffill')

# 4. Add time-based features
df['Hour'] = df['DateHour'].dt.hour
df['DayOfWeek'] = df['DateHour'].dt.dayofweek
df['Month'] = df['DateHour'].dt.month
df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)

# 5. Label crime occurrences
crime_df['Key'] = list(zip(crime_df['DateHour'], crime_df['LatGrid'], crime_df['LonGrid']))
df['Key'] = list(zip(df['DateHour'], df['LatGrid'], df['LonGrid']))
df['CrimeOccurred'] = df['Key'].isin(set(crime_df['Key'])).astype(int)


# 6. Arrest signals using vectorized aggregation
crime_df_arrests = crime_df[crime_df['Arrest'] == True].copy()
arrests_grouped = crime_df_arrests.groupby(['LatGrid', 'LonGrid', 'DateHour']).size().reset_index(name='ArrestCount')

# Expand arrest counts over 7-day and 30-day windows
df = df.sort_values(['LatGrid', 'LonGrid', 'DateHour'])
arrests_grouped = arrests_grouped.sort_values(['LatGrid', 'LonGrid', 'DateHour'])

# Merge and roll
df = df.merge(arrests_grouped, on=['DateHour', 'LatGrid', 'LonGrid'], how='left')
df['ArrestCount'] = df['ArrestCount'].fillna(0)

# Rolling sums
df['RecentArrests'] = df.groupby(['LatGrid', 'LonGrid'])['ArrestCount'].transform(lambda x: x.rolling(window=168, min_periods=1).sum())  # 168 = 7 days * 24 hours
df['RepeatOffenderSignal'] = df.groupby(['LatGrid', 'LonGrid'])['ArrestCount'].transform(lambda x: x.rolling(window=720, min_periods=1).sum())  # 30 days * 24


# 7. Drop NA weather rows
df.dropna(subset=['temp', 'rhum', 'prcp', 'snow', 'wspd'], inplace=True)


# 8. Train model
features = [
    'Hour', 'DayOfWeek', 'Month', 'IsWeekend',
    'temp', 'rhum', 'prcp', 'snow', 'wspd',
    'RecentArrests', 'RepeatOffenderSignal'
]
X = df[features]
y = df['CrimeOccurred']
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)
clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
clf.fit(X_train, y_train)
joblib.dump(clf, "crime_predictor_model.pkl")


# 9. Evaluation
y_pred = clf.predict(X_test)
y_proba = clf.predict_proba(X_test)[:, 1]
print(classification_report(y_test, y_pred, zero_division=0))
print("ROC-AUC Score:", roc_auc_score(y_test, y_proba))

# 10. Feature importance
feat_imp = pd.Series(clf.feature_importances_, index=features).sort_values(ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(x=feat_imp, y=feat_imp.index)
plt.title("Feature Importance")
plt.tight_layout()
plt.show()

# 11. Predict next 12 hours per location
future_start = df['DateHour'].max() + pd.Timedelta(hours=1)
future_hours = pd.date_range(start=future_start, periods=12, freq='H')
locations = df[['LatGrid', 'LonGrid']].drop_duplicates()
future_grid = pd.MultiIndex.from_product([future_hours, locations['LatGrid'], locations['LonGrid']], names=['DateHour', 'LatGrid', 'LonGrid'])
df_future = pd.DataFrame(index=future_grid).reset_index()
weather_future = Hourly(station, future_start, future_hours.max()).fetch().reset_index()
weather_future['DateHour'] = weather_future['time'].dt.floor('h')
df_future = df_future.merge(weather_future, on='DateHour', how='left')
df_future['Hour'] = df_future['DateHour'].dt.hour
df_future['DayOfWeek'] = df_future['DateHour'].dt.dayofweek
df_future['Month'] = df_future['DateHour'].dt.month
df_future['IsWeekend'] = df_future['DayOfWeek'].isin([5, 6]).astype(int)

# Merge recent arrest counts
combined = pd.concat([df[['DateHour', 'LatGrid', 'LonGrid', 'ArrestCount']], df_future[['DateHour', 'LatGrid', 'LonGrid']]], ignore_index=True)
combined = combined.sort_values(['LatGrid', 'LonGrid', 'DateHour'])
combined['ArrestCount'] = combined['ArrestCount'].fillna(0)
combined['RecentArrests'] = combined.groupby(['LatGrid', 'LonGrid'])['ArrestCount'].transform(lambda x: x.rolling(window=168, min_periods=1).sum())
combined['RepeatOffenderSignal'] = combined.groupby(['LatGrid', 'LonGrid'])['ArrestCount'].transform(lambda x: x.rolling(window=720, min_periods=1).sum())

# Filter only future window from combined df
merged_future = combined.merge(df_future, on=['DateHour', 'LatGrid', 'LonGrid'], how='right')

# Drop NAs
df_future_final = merged_future.dropna(subset=['temp', 'rhum', 'prcp', 'snow', 'wspd'])

# Predict
X_future = df_future_final[features]
df_future_final['PredictedCrime'] = clf.predict(X_future)
df_future_final['Probability'] = clf.predict_proba(X_future)[:, 1]

print("\nTop locations with highest predicted crime probability in next 12 hours:")
print(df_future_final[df_future_final['PredictedCrime'] == 1][['DateHour', 'LatGrid', 'LonGrid', 'Probability']].sort_values(by='Probability', ascending=False))



