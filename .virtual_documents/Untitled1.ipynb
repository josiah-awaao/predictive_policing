# pip install meteostat tqdm joblib
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from meteostat import Hourly, Point
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from tqdm import tqdm

tqdm.pandas()

# 1. Load crime dataset
crime_df = pd.read_csv("Crimes_-_2001_to_Present_20250410.csv", low_memory=False)
crime_df['Date'] = pd.to_datetime(crime_df['Date'])
crime_df = crime_df[['Date', 'Arrest', 'Latitude', 'Longitude', 'Community Area']]
crime_df.dropna(subset=['Date', 'Latitude', 'Longitude'], inplace=True)
crime_df['DateHour'] = crime_df['Date'].dt.floor('h')
crime_df['LatGrid'] = crime_df['Latitude'].round(2)
crime_df['LonGrid'] = crime_df['Longitude'].round(2)






# 6. Arrest signals using vectorized aggregation
crime_df_arrests = crime_df[crime_df['Arrest'] == True].copy()
arrests_grouped = crime_df_arrests.groupby(['LatGrid', 'LonGrid', 'DateHour']).size().reset_index(name='ArrestCount')
df = df.sort_values(['LatGrid', 'LonGrid', 'DateHour'])
arrests_grouped = arrests_grouped.sort_values(['LatGrid', 'LonGrid', 'DateHour'])
df = df.merge(arrests_grouped, on=['DateHour', 'LatGrid', 'LonGrid'], how='left')
df['ArrestCount'] = df['ArrestCount'].fillna(0)
df['RecentArrests'] = df.groupby(['LatGrid', 'LonGrid'])['ArrestCount'].transform(lambda x: x.rolling(window=168, min_periods=1).sum())
df['RepeatOffenderSignal'] = df.groupby(['LatGrid', 'LonGrid'])['ArrestCount'].transform(lambda x: x.rolling(window=720, min_periods=1).sum())



# 7. Final feature check with verbose report
features = [
    'Hour', 'DayOfWeek', 'Month', 'IsWeekend',
    'temp', 'rhum', 'prcp', 'snow', 'wspd',
    'RecentArrests', 'RepeatOffenderSignal'
]

# Print how many missing per feature
missing = df[features].isnull().sum()
print("🔍 Missing values before drop:")
print(missing[missing > 0])

# Drop features that are completely empty
features = [f for f in features if df[f].notna().sum() > 0]
print("✅ Features retained for model:", features)

# Fill what we can
df[features] = df[features].ffill().bfill()

# Drop any row still missing something important
df = df.dropna(subset=features)
print("🔎 Remaining rows after cleaning:", df.shape)

if df.empty:
    raise ValueError("Dataframe is empty after cleaning. Try checking weather coverage or use a narrower date range.")


# 8. Train model
X = df[features]
y = df['CrimeOccurred']

# Force full dataset training — don't rebalance
if len(y.unique()) < 2:
    print("❌ Only one class present. Not enough variability to train.")
    clf = None
else:
    print(f"✅ Proceeding with full dataset: {len(y)} records")
    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)
    clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
    clf.fit(X_train, y_train)
    joblib.dump(clf, "crime_predictor_model.pkl")


# 9. Evaluation
if clf is not None:
    y_pred = clf.predict(X_test)
    y_proba = clf.predict_proba(X_test)[:, 1]
    print(classification_report(y_test, y_pred, zero_division=0))
    print("ROC-AUC Score:", roc_auc_score(y_test, y_proba))

    # 10. Feature importance
    feat_imp = pd.Series(clf.feature_importances_, index=features).sort_values(ascending=False)
    plt.figure(figsize=(10, 6))
    sns.barplot(x=feat_imp, y=feat_imp.index)
    plt.title("Feature Importance")
    plt.tight_layout()
    plt.show()
else:
    print("⚠️ Skipping evaluation: Model not trained due to lack of class variation.")



# 10. Feature importance
feat_imp = pd.Series(clf.feature_importances_, index=features).sort_values(ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(x=feat_imp, y=feat_imp.index)
plt.title("Feature Importance")
plt.tight_layout()
plt.show()



